{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim import Adam\n",
    "from torchvision import datasets\n",
    "from torchvision import models\n",
    "from torchvision.models import DenseNet169_Weights\n",
    "\n",
    "import random\n",
    "\n",
    "from colorama import Fore, Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1945e0fe4e3063",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# colorama\n",
    "red = Fore.RED\n",
    "green = Fore.GREEN\n",
    "blue = Fore.BLUE\n",
    "yellow = Fore.YELLOW\n",
    "cyan = Fore.CYAN\n",
    "\n",
    "reset = Style.RESET_ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a61261245844a6f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "d = \".../Rocks/\"\n",
    "\n",
    "loc = \"PyDL_C\"\n",
    "\n",
    "# Sub-Categorized data\n",
    "train_dir = d + loc + \"/data/train\"\n",
    "test_dir = d + loc + \"/data/test\"\n",
    "valid_dir = d + loc + \"/data/validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbff190b55960ca",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setting the seed\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "print(f'{blue}Global seed set to : {yellow}{seed}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6c27f7785f5831",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data transformations training set\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Data transformations for validation and test sets\n",
    "transform_common = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Image dataset\n",
    "dataset_train = datasets.ImageFolder(root=train_dir, transform=transform_train)\n",
    "dataset_test = datasets.ImageFolder(root=test_dir, transform=transform_common)\n",
    "dataset_valid = datasets.ImageFolder(root=valid_dir, transform=transform_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87bb6dc2ef941be",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "max_epoch = 50\n",
    "batch_size = 16\n",
    "learning_rate = [0.001, 0.0001, 1e-05]\n",
    "weight_decay = [1e-07, 1e-08, 1e-09]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4d9c9ef528135a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data loaders\n",
    "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size, shuffle=False)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset_valid, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44ca2a5383ee93e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'{blue}Device: {yellow}{device}{reset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2bac6d4499439b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid_search = []\n",
    "\n",
    "for lr in learning_rate:\n",
    "    for wd in weight_decay:\n",
    "        \n",
    "        # ---------------------------------------------------------------------------    \n",
    "        # DenseNet\n",
    "        model = models.densenet169(weights=DenseNet169_Weights.IMAGENET1K_V1)\n",
    "\n",
    "        num_classes = len(dataset_train.classes)\n",
    "\n",
    "        # change the last layer of the model\n",
    "        num_ftrs = model.classifier.in_features\n",
    "        model.classifier = nn.Linear(in_features=num_ftrs, out_features=num_classes)\n",
    "        \n",
    "        model.to(device)\n",
    "        \n",
    "        # ---------------------------------------------------------------------------\n",
    "        \n",
    "        # Loss\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Optimizer\n",
    "        optimizer = Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "        \n",
    "        # ---------------------------------------------------------------------------\n",
    "        \n",
    "        # best model score\n",
    "        max_score = 0\n",
    "        \n",
    "        print(f'{blue}Training started for :- {yellow}Learning Rate : {lr} | Weight Decay : {wd}{reset}\\n')\n",
    "        \n",
    "        for epoch in range(max_epoch):\n",
    "            model.train()\n",
    "        \n",
    "            # Metrics initialization\n",
    "            running_loss = 0.0\n",
    "            num_correct = 0\n",
    "        \n",
    "            # TRAINING\n",
    "            for i, data in enumerate(train_loader, 0):\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "        \n",
    "                # Predictions | forward pass | OUTPUT\n",
    "                outputs = model(inputs)\n",
    "                # Loss | backward pass | GRADIENT\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "                # Metrics\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                # Count correct predictions\n",
    "                num_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "            # ---------------------------------------------------------------------------\n",
    "            # Training loss\n",
    "            train_lss = running_loss / len(train_loader)\n",
    "        \n",
    "            # Training accuracy\n",
    "            train_accuracy = 100 * num_correct / len(train_loader.dataset)\n",
    "            # ---------------------------------------------------------------------------\n",
    "        \n",
    "            model.eval()\n",
    "            correct = 0\n",
    "            valid_loss = 0\n",
    "        \n",
    "            # VALIDATION\n",
    "            with torch.no_grad():\n",
    "                for data in valid_loader:\n",
    "                    inputs, labels = data\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "                    # Predictions\n",
    "                    outputs = model(inputs)\n",
    "                    # Count correct predictions\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                    # Loss\n",
    "                    valid_loss += criterion(outputs, labels).item()\n",
    "        \n",
    "            # --------------------------------------------------------------------------\n",
    "            #Validation loss\n",
    "            val_lss = valid_loss / len(valid_loader)\n",
    "        \n",
    "            # Validation accuracy\n",
    "            val_accuracy = 100 * correct / len(valid_loader.dataset)\n",
    "            # --------------------------------------------------------------------------\n",
    "        \n",
    "            if epoch % 5 == 0:\n",
    "                print(f\"{cyan}EPOCH {epoch+1}{reset}\\t Loss: {red}{train_lss}{reset}\\t Validation Accuracy: {red}{val_accuracy}%{reset}\\t Training Accuracy: {red}{train_accuracy}%\")\n",
    "            \n",
    "            # Save the best model\n",
    "            if val_accuracy > max_score:\n",
    "                max_score = val_accuracy\n",
    "                name = f'T__{lr}__{wd}.pth'\n",
    "                path = d + loc + '/models/' + name\n",
    "                torch.save(model.state_dict(), path)\n",
    "        \n",
    "        print(f'{yellow}Training finished for : {lr} | {wd}{reset}\\n\\n')\n",
    "        \n",
    "        # Results saved\n",
    "        ll = [lr, wd, max_score]\n",
    "        grid_search.append(ll)\n",
    "        \n",
    "        # Save the Final model\n",
    "        name = f'F__{lr}__{wd}.pth'\n",
    "        path = d + loc + '/models/' + name\n",
    "        torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6105fd62d576fb4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Final models results\n",
    "k = 0\n",
    "\n",
    "for lr in learning_rate:\n",
    "    for wd in weight_decay:        \n",
    "        # --------------------------------------------------------------------------\n",
    "        # TESTING on BEST Model\n",
    "        name_T = f'T__{lr}__{wd}.pth'\n",
    "        \n",
    "        b_model = models.densenet169(weights=DenseNet169_Weights.IMAGENET1K_V1)\n",
    "        num_classes = len(dataset_train.classes)\n",
    "        num_ftrs = b_model.classifier.in_features\n",
    "        b_model.classifier = nn.Linear(in_features=num_ftrs, out_features=num_classes)\n",
    "        b_model.load_state_dict(torch.load(d + loc + '/models/' + name_T))        \n",
    "        \n",
    "        total = 0      \n",
    "        correct = 0      \n",
    "        \n",
    "        b_model.eval()\n",
    "        b_model.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(test_loader, 0):\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "                # Best Model\n",
    "                # Predictions | forward pass | OUTPUT\n",
    "                outputs = b_model(inputs)\n",
    "                # Count correct predictions\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                    \n",
    "        grid_search[k].append(100 * correct / total)\n",
    "        \n",
    "        # --------------------------------------------------------------------------\n",
    "        # Testing on FINAL Model\n",
    "        name_F = f'F__{lr}__{wd}.pth'\n",
    "        \n",
    "        b_model = models.densenet169(weights=DenseNet169_Weights.IMAGENET1K_V1)\n",
    "        num_ftrs = b_model.classifier.in_features\n",
    "        b_model.classifier = nn.Linear(in_features=num_ftrs, out_features=num_classes)\n",
    "        b_model.load_state_dict(torch.load(d + loc + '/models/' + name_F))        \n",
    "        \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        b_model.eval()\n",
    "        b_model.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(test_loader, 0):\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "                # Predictions | forward pass | OUTPUT\n",
    "                outputs = b_model(inputs)\n",
    "                # Count correct predictions\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "        grid_search[k].append(100 * correct / total)\n",
    "        \n",
    "        # next in grid_search\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93923fc6f3a51287",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Results\n",
    "print(f'{blue}Grid Search Results{reset}\\n')\n",
    "\n",
    "best_result = 0\n",
    "best = []\n",
    "\n",
    "for i in range(len(grid_search)):\n",
    "    print(f'Learning Rate: {grid_search[i][0]} | Weight Decay: {grid_search[i][1]} | Validation Accuracy: {grid_search[i][2]} || Test : Best Model: {grid_search[i][3]} & Final Model: {grid_search[i][4]}')\n",
    "    \n",
    "    if grid_search[i][3] > best_result or grid_search[i][4] > best_result:\n",
    "        best_result = grid_search[i][3]\n",
    "        best = grid_search[i]\n",
    "       \n",
    "print(f'\\n\\n{red}Best Result : lr = {best[0]} | wd = {best[1]} | val_acc = {best[2]} || Test : Best Model: {best[3]} & Final Model: {best[4]}{reset}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
