{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim import Adam\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torchvision import models\n",
    "from torchvision.models import MobileNet_V3_Small_Weights\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "from colorama import Fore, Style\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, roc_curve, auc\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce8889e3a9884eb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# colorama\n",
    "red = Fore.RED\n",
    "green = Fore.GREEN\n",
    "blue = Fore.BLUE\n",
    "yellow = Fore.YELLOW\n",
    "cyan = Fore.CYAN\n",
    "\n",
    "reset = Style.RESET_ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab01e7056abf50f1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "d = \".../Rock/\"\n",
    "\n",
    "fld = 'PyDL_C'\n",
    "\n",
    "# Sub-Categorized data\n",
    "train_dir = d + \"k_fold_data/train\"\n",
    "test_dir = d + \"k_fold_data/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125fd26916d78f9d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setting the seed\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "print(f'{blue}Global seed set to : {yellow}{seed}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3976ab324633ba4a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_dimen = (256, 256)\n",
    "bs = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f340258319ab11",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preprocessing | get the data mean and std for normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(img_dimen),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "calc_ms = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "loader_ms = torch.utils.data.DataLoader(dataset=calc_ms, batch_size=bs, shuffle=False)\n",
    "\n",
    "mean_calc = 0\n",
    "std_calc = 0\n",
    "total_images = 0\n",
    "\n",
    "for images, _ in tqdm(loader_ms):\n",
    "    batch_samples = images.size(0)\n",
    "    images = images.view(batch_samples, images.size(1), -1)\n",
    "    mean_calc += images.mean(2).sum(0)\n",
    "    std_calc += images.std(2).sum(0)\n",
    "    total_images += batch_samples\n",
    "\n",
    "mean_calc /= total_images\n",
    "std_calc /= total_images\n",
    "\n",
    "print(f'{blue}mean: {yellow}{mean_calc}')\n",
    "print(f'{blue}std: {yellow}{std_calc}{reset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3949e354272f537",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ImageNet Normalization\n",
    "#mean_calc = [0.485, 0.456, 0.406]\n",
    "#std_calc = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7938cff18faa992c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data transformations training set\n",
    "transform_all = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(img_dimen),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_calc, std_calc)\n",
    "])\n",
    "\n",
    "# Data transformations for validation and test sets\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(img_dimen),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_calc, std_calc)\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root=train_dir, transform=transform_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f2d84e9b5ad7de",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test set\n",
    "batch_size = bs\n",
    "dataset_test = datasets.ImageFolder(test_dir, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1916ccd82073a442",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Classes\n",
    "num_classes = len(dataset.classes)\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'{blue}Device: {yellow}{device}{reset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad5eabafe579e6b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "fold = 5\n",
    "max_epoch = 30\n",
    "batch_size = 16\n",
    "learningRate = 0.0001\n",
    "WeightDecay = 1e-08\n",
    "\n",
    "# All Information\n",
    "print(f'{blue}Fold: {yellow}{fold}{reset}')\n",
    "print(f'{blue}Epochs: {yellow}{max_epoch}{reset}')\n",
    "print(f'{blue}Batch size: {yellow}{batch_size}{reset}')\n",
    "print(f'{blue}Learning rate: {yellow}{learningRate}{reset}')\n",
    "print(f'{blue}Weight decay: {yellow}{WeightDecay}{reset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5692bfc8af7448bf",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# k-fold cross-validation\n",
    "kf = KFold(n_splits=fold, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbc3f3a27ac4628",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# K fold cross-validation\n",
    "\n",
    "# Define your train and validation scores for all folds\n",
    "# Loss metrics\n",
    "train_loss_all = []\n",
    "val_loss_all = []\n",
    "# Accuracy metrics\n",
    "train_acc_all = []\n",
    "val_acc_all = []\n",
    "\n",
    "# validation accuracy for calculating average\n",
    "fold_val_acc = []\n",
    "\n",
    "# Loop over each fold\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(dataset)):    \n",
    "    print(f'{yellow}\\n##############################################')\n",
    "    print(f'{green}                   FOLD {fold + 1}')\n",
    "    print(f'{yellow}##############################################{reset}')\n",
    "\n",
    "    # Define your train and validation datasets\n",
    "    train_dataset = torch.utils.data.Subset(dataset, train_index)\n",
    "    val_dataset = torch.utils.data.Subset(dataset, val_index)\n",
    "\n",
    "    # Define your train and validation dataloaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # -----------------------------------------------------------------------------------\n",
    "    \n",
    "    # MobileNet V3 Small\n",
    "    model = models.mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "\n",
    "    num_classes = len(dataset.classes)\n",
    "\n",
    "    num_ftrs = model.classifier[3].in_features\n",
    "    model.classifier[3] = nn.Linear(in_features=num_ftrs, out_features=num_classes)\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    # -----------------------------------------------------------------------------------\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=learningRate, weight_decay=WeightDecay)\n",
    "    \n",
    "    # -----------------------------------------------------------------------------------  \n",
    "    \n",
    "    # TRAINING\n",
    "    # loss metrics\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    # Accuracy metrics\n",
    "    train_acc = []\n",
    "    val_acc = []\n",
    "\n",
    "    # Max score for the current fold\n",
    "    max_curr_fold = 0\n",
    "\n",
    "    # Loop over each epoch\n",
    "    for epoch in range(max_epoch):\n",
    "        model.train()\n",
    "\n",
    "        # Metrics initialization\n",
    "        running_loss = 0.0\n",
    "        num_correct = 0\n",
    "\n",
    "        # TRAINING\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Predictions | forward pass | OUTPUT\n",
    "            outputs = model(inputs)\n",
    "            # Loss | backward pass | GRADIENT\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Metrics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            # Count correct predictions\n",
    "            num_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "        # ---------------------------------------------------------------------------\n",
    "        # Training loss\n",
    "        train_lss = running_loss / len(train_loader)\n",
    "        train_loss.append(train_lss)\n",
    "\n",
    "        # Training accuracy\n",
    "        train_accuracy = 100 * num_correct / len(train_loader.dataset)\n",
    "        train_acc.append(train_accuracy)\n",
    "        # ---------------------------------------------------------------------------\n",
    "\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        valid_loss = 0\n",
    "\n",
    "        # VALIDATION\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                # Predictions\n",
    "                outputs = model(inputs)\n",
    "                # Count correct predictions\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                # Loss\n",
    "                valid_loss += criterion(outputs, labels).item()\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        #Validation loss\n",
    "        val_lss = valid_loss / len(val_loader)\n",
    "        val_loss.append(val_lss)\n",
    "\n",
    "        # Validation accuracy\n",
    "        val_accuracy = 100 * correct / len(val_loader.dataset)\n",
    "        val_acc.append(val_accuracy)\n",
    "        \n",
    "        # --------------------------------------------------------------------------\n",
    "        \n",
    "        print(f'{cyan}\\nEPOCH {epoch + 1}{reset}')\n",
    "        print(f\"Loss: {red}{train_lss}{reset}, Validation Accuracy: {red}{val_accuracy}%{reset}, Training Accuracy: {red}{train_accuracy}%\")\n",
    "        \n",
    "        # Save the best model of each fold\n",
    "        if val_accuracy > max_curr_fold:\n",
    "            max_curr_fold = val_accuracy\n",
    "            ff = fold + 1\n",
    "            path = d + fld + '/models/MobileNetV3_s_fold_T_' + str(ff) +'.pth'\n",
    "            torch.save(model.state_dict(), path)\n",
    "            print(f'{green}Improvement! Model saved!{reset}')\n",
    "    \n",
    "    # save last model\n",
    "    ff = fold + 1\n",
    "    path = d + fld + '/models/MobileNetV3_s_fold_F_' + str(ff) +'.pth'\n",
    "    torch.save(model.state_dict(), path)\n",
    "    \n",
    "    # ------------------------------------------------------------------------------\n",
    "    \n",
    "    # metrics for graph for current fold\n",
    "    train_loss_all.append(train_loss)\n",
    "    val_loss_all.append(val_loss)\n",
    "    \n",
    "    train_acc_all.append(train_acc)\n",
    "    val_acc_all.append(val_acc)\n",
    "    \n",
    "    # the highest validation accuracy of each fold       \n",
    "    fold_val_acc.append(max_curr_fold)\n",
    "    \n",
    "    # ------------------------------------------------------------------------------\n",
    "        \n",
    "print(f'{yellow}\\nTraining finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf9776c9002ab93",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Graph of training and validation: loss and accuracy | dual plots for each fold\n",
    "fig, axis = plt.subplots(5, 2, figsize=(20, 40))\n",
    "\n",
    "for i in range(5):\n",
    "    # Loss plot\n",
    "    axis[i, 0].set_title(\"Fold \" + str(i+1) + \": Loss\")\n",
    "    axis[i, 0].plot(val_loss_all[i], color='red', label='Validation loss', linestyle='dashed')\n",
    "    axis[i, 0].plot(train_loss_all[i], color='orange', label='Training loss')\n",
    "    axis[i, 0].legend()\n",
    "    axis[i, 0].set_xlabel(\"Iterations\")\n",
    "    axis[i, 0].set_ylabel(\"Loss\")\n",
    "\n",
    "    # Accuracy plot\n",
    "    axis[i, 1].set_title(\"Fold \" + str(i+1) + \": Accuracy\")\n",
    "    axis[i, 1].plot(val_acc_all[i], color='red', label='Validation accuracy', linestyle='dashed')\n",
    "    axis[i, 1].plot(train_acc_all[i], color='orange', label='Training accuracy')\n",
    "    axis[i, 1].legend()\n",
    "    axis[i, 1].set_xlabel(\"Iterations\")\n",
    "    axis[i, 1].set_ylabel(\"Accuracy\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28834fb3dc91688e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Graph of training and validation: loss and accuracy | single plot for all folds\n",
    "fig, axis = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "acc_mean = []\n",
    "loss_mean = []\n",
    "\n",
    "for i in range(5):\n",
    "    acc_mean.append(sum(val_acc_all[i]) / len(val_acc_all[i]))\n",
    "    loss_mean.append(sum(val_loss_all[i]) / len(val_loss_all[i]))\n",
    "    \n",
    "acc_std = []\n",
    "loss_std = []\n",
    "\n",
    "for i in range(5):\n",
    "    acc_std.append(np.std(val_acc_all[i]))\n",
    "    loss_std.append(np.std(val_loss_all[i]))\n",
    "    \n",
    "# Loss plot\n",
    "axis[0].set_title(\"Loss\")\n",
    "axis[0].errorbar(range(1, 6), loss_mean, yerr=loss_std, color='red', label='Validation loss', linestyle='dashed')\n",
    "axis[0].plot(range(1, 6), loss_mean, color='orange', label='Training loss')\n",
    "axis[0].legend()\n",
    "axis[0].set_xlabel(\"Folds\")\n",
    "axis[0].set_ylabel(\"Loss\")\n",
    "\n",
    "# Accuracy plot\n",
    "axis[1].set_title(\"Accuracy\")\n",
    "axis[1].errorbar(range(1, 6), acc_mean, yerr=acc_std, color='red', label='Validation accuracy', linestyle='dashed')\n",
    "axis[1].plot(range(1, 6), acc_mean, color='orange', label='Training accuracy')\n",
    "axis[1].legend()\n",
    "axis[1].set_xlabel(\"Folds\")\n",
    "axis[1].set_ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e38c9b9a28cd4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TESTING on BEST Model (fold wise)\n",
    "\n",
    "# All Accuracy for average calculation\n",
    "acc = []\n",
    "y_pred_ll = []\n",
    "y_true_ll = []\n",
    "\n",
    "for f in range(5):\n",
    "    acc_val = 0\n",
    "    acc_final = 0\n",
    "    best = 0\n",
    "    \n",
    "    y_pred_T = []\n",
    "    y_true_T = []\n",
    "    \n",
    "    y_pred_F = []\n",
    "    y_true_F = []\n",
    "    \n",
    "    # -----------------------------------------------------------------------------------------\n",
    "    # Training Model\n",
    "    b_model = models.mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "    num_ftrs = b_model.classifier[3].in_features\n",
    "    b_model.classifier[3] = nn.Linear(in_features=num_ftrs, out_features=num_classes)\n",
    "    \n",
    "    path = d + 'PyDL_Mango/models/MobileNetV3_s_fold_T_' + str(f+1) +'.pth'\n",
    "    b_model.load_state_dict(torch.load(path))\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    b_model.eval()\n",
    "    b_model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Predictions | forward pass | OUTPUT\n",
    "            outputs = b_model(inputs)\n",
    "            # Count correct predictions\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # for classification report\n",
    "            y_pred_T.extend(predicted.tolist())\n",
    "            y_true_T.extend(labels.tolist())\n",
    "    \n",
    "    # Validation best model accuracy    \n",
    "    acc_val = 100 * correct / total\n",
    "    \n",
    "    # -----------------------------------------------------------------------------------------\n",
    "    # Final Model\n",
    "    f_model = models.mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "    num_ftrs = f_model.classifier[3].in_features\n",
    "    f_model.classifier[3] = nn.Linear(in_features=num_ftrs, out_features=num_classes)\n",
    "    \n",
    "    path = d + 'PyDL_Mango/models/MobileNetV3_s_fold_F_' + str(f+1) +'.pth'\n",
    "    f_model.load_state_dict(torch.load(path))\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    f_model.eval()\n",
    "    f_model.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Predictions | forward pass | OUTPUT\n",
    "            outputs = f_model(inputs)\n",
    "            # Count correct predictions\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # for classification report\n",
    "            y_pred_F.extend(predicted.tolist())\n",
    "            y_true_F.extend(labels.tolist())\n",
    "       \n",
    "    # Final model accuracy     \n",
    "    acc_final = 100 * correct / total\n",
    "    \n",
    "    # -----------------------------------------------------------------------------------------\n",
    "    if acc_val > acc_final:\n",
    "        y_pred_ll.append(y_pred_T)\n",
    "        y_true_ll.append(y_true_T)\n",
    "    else:\n",
    "        y_pred_ll.append(y_pred_F)\n",
    "        y_true_ll.append(y_true_F)\n",
    "        \n",
    "    # -----------------------------------------------------------------------------------------\n",
    "    best = max(acc_val, acc_final)\n",
    "    \n",
    "    # fold\n",
    "    print(f\"{green}\\nFold {f+1}:\")\n",
    "    print(f\"{blue}Validation Accuracy: {red}{fold_val_acc[f]}%\")\n",
    "    print(f\"{blue}Test Accuracy: {red}{best}%\")\n",
    "    acc.append(best)\n",
    " \n",
    "print(f\"{blue}\\n\\nAverage Validation Accuracy: {red}{sum(fold_val_acc) / len(fold_val_acc)}%\")  \n",
    "print(f\"{blue}Average Test Accuracy: {red}{sum(acc) / len(acc)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35e92ad43a00205",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "for i in range(5):\n",
    "    print(f\"{green}\\nFold {i+1}:\")\n",
    "    print(f\"{blue}Classification Report:\")\n",
    "    print(classification_report(y_true_ll[i], y_pred_ll[i], target_names=dataset_test.classes), end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dedc696e5f3842",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "acc_metric = []\n",
    "\n",
    "for i in range(5):\n",
    "    acc = accuracy_score(y_true_ll[i], y_pred_ll[i])\n",
    "    acc_metric.append(acc)\n",
    "    print(f\"{green}\\nFold {i+1}:\")\n",
    "    print(f\"{blue}Accuracy: {red}{acc}\")\n",
    "    \n",
    "print(f\"{blue}\\n\\nAverage Accuracy: {red}{sum(acc_metric) / len(acc_metric)}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78895fcbe66ef587",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# F1 Score\n",
    "f_score_metric = []\n",
    "\n",
    "for i in range(5):\n",
    "    f_score = f1_score(y_true_ll[i], y_pred_ll[i], average='macro')\n",
    "    f_score_metric.append(f_score)\n",
    "    print(f\"{green}\\nFold {i+1}:\")\n",
    "    print(f\"{blue}F1 Score: {red}{f_score}\")\n",
    "    \n",
    "print(f\"{blue}\\n\\nAverage F1 Score: {red}{sum(f_score_metric) / len(f_score_metric)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840a3705cf0fd447",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Precision\n",
    "precision_metric = []\n",
    "\n",
    "for i in range(5):\n",
    "    precision = precision_score(y_true_ll[i], y_pred_ll[i], average='macro')\n",
    "    precision_metric.append(precision)\n",
    "    print(f\"{green}\\nFold {i+1}:\")\n",
    "    print(f\"{blue}Precision: {red}{precision}\")\n",
    "    \n",
    "print(f\"{blue}\\n\\nAverage Precision: {red}{sum(precision_metric) / len(precision_metric)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a2b497c818bf42",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Recall | Sensitivity\n",
    "sen_metric = []\n",
    "\n",
    "for i in range(5):\n",
    "    sen = recall_score(y_true_ll[i], y_pred_ll[i], average='macro')\n",
    "    sen_metric.append(sen)\n",
    "    print(f\"{green}\\nFold {i+1}:\")\n",
    "    print(f\"{blue}Recall/Sensitivity: {red}{sen}\")\n",
    "    \n",
    "print(f\"{blue}\\n\\nAverage Recall/Sensitivity: {red}{sum(sen_metric) / len(sen_metric)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d66c9af89cd4cf5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tp_calc (y_true, y_pred, class_label):\n",
    "    tp = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] == class_label and y_pred[i] == class_label:\n",
    "            tp += 1\n",
    "    return tp\n",
    "    \n",
    "def tn_calc (y_true, y_pred, class_label):\n",
    "    tn = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] != class_label and y_pred[i] != class_label:\n",
    "            tn += 1\n",
    "    return tn\n",
    "    \n",
    "def fp_calc (y_true, y_pred, class_label):\n",
    "    fp = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] != class_label and y_pred[i] == class_label:\n",
    "            fp += 1\n",
    "    return fp\n",
    "    \n",
    "def fn_calc (y_true, y_pred, class_label):\n",
    "    fn = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] == class_label and y_pred[i] != class_label:\n",
    "            fn += 1\n",
    "    return fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14012f45ae93d459",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_specificity(y_true, y_pred, class_index):\n",
    "    # Convert y_true and y_pred to numpy arrays if they are lists\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # Identify true positive, false positive, true negative, and false negative counts\n",
    "    #true_positive = np.sum((y_true == class_index) & (y_pred == class_index))\n",
    "    false_positive = np.sum((y_true != class_index) & (y_pred == class_index))\n",
    "    true_negative = np.sum((y_true != class_index) & (y_pred != class_index))\n",
    "    #false_negative = np.sum((y_true == class_index) & (y_pred != class_index))\n",
    "\n",
    "    # Calculate specificity\n",
    "    specificity = true_negative / (true_negative + false_positive)\n",
    "\n",
    "    return specificity\n",
    "\n",
    "def calculate_multi_class_specificity(y_true, y_pred):\n",
    "    num_classes = len(np.unique(y_true))\n",
    "    specificity_scores = []\n",
    "\n",
    "    for class_index in range(num_classes):\n",
    "        specificity = calculate_specificity(y_true, y_pred, class_index)\n",
    "        specificity_scores.append(specificity)\n",
    "\n",
    "    # Calculate the average specificity across all classes\n",
    "    average_specificity = np.mean(specificity_scores)\n",
    "\n",
    "    return average_specificity, specificity_scores\n",
    "\n",
    "average_specificity, specificity_scores = calculate_multi_class_specificity(y_true_ll[2], y_pred_ll[2])\n",
    "\n",
    "print(f'Average Specificity: {average_specificity}')\n",
    "print('Specificity for Each Class:', specificity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82f67e164a238c2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "for i in range(5):\n",
    "    # plot confusion matrix on 10, 10 figure with a blue color map\n",
    "    cm = confusion_matrix(y_true_ll[i], y_pred_ll[i])\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=dataset_test.classes)\n",
    "    disp.plot(cmap=plt.cm.Blues, xticks_rotation='vertical')\n",
    "    plt.title(\"Fold \" + str(i+1))\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
